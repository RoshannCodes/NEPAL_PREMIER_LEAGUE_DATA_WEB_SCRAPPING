{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(r'C:\\\\Users\\\\Roshan Koirala\\\\Desktop\\\\FUN with GPT\\\\NEPAL PREMIER LEAGUE\\\\NPL NEW TRY\\\\geckodriver-v0.35.0-win64 (1)\\\\geckodriver.exe')\n",
    "\n",
    "driver = webdriver.Firefox(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "try:\n",
    "    url = \"https://www.espncricinfo.com/series/nepal-premier-league-2024-25-1462594/match-schedule-fixtures-and-results\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    npl_matches = []\n",
    "    \n",
    "    for _ in range(5):  \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='nepal-premier-league'][href*='full-scorecard']\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and href not in npl_matches:\n",
    "                npl_matches.append(href)\n",
    "    \n",
    "    for match in npl_matches:\n",
    "        print(match)\n",
    "    print(f\"\\nTotal NPL matches: {len(npl_matches)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_team_names(driver):\n",
    "    team_elements = driver.find_elements(\n",
    "        By.CSS_SELECTOR, \".ds-text-title-xs.ds-font-bold.ds-capitalize\"\n",
    "    )\n",
    "\n",
    "    team_names = [team.text.strip() for team in team_elements]\n",
    "\n",
    "    if len(team_names) >= 2:\n",
    "        return team_names[0], team_names[1]\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_match_details(driver, match_url):\n",
    "    driver.get(match_url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    table = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'ds-table')))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    match_details = {}\n",
    "    rows = soup.find_all('tr')\n",
    "    \n",
    "    team1, team2 = extract_team_names(driver)\n",
    "    team1 = team1.split(' ')[:2]\n",
    "    team2 = team2.split(' ')[:2]\n",
    "    \n",
    "    match_details['Team 1'] = ' '.join(team1)\n",
    "    match_details['Team 2'] = ' '.join(team2) \n",
    "       \n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) == 2:  # two columns (label and value)\n",
    "            label = cols[0].text.strip()\n",
    "            value = cols[1].text.strip()\n",
    "            \n",
    "            if 'Toss' in label:\n",
    "                toss_winner_name = value.split(' ')[:2]\n",
    "                match_details['Toss_Winner'] = ' '.join(toss_winner_name)  \n",
    "                match_details['Toss_Decision'] = value.split(' ')[-2]\n",
    "            # elif 'Series' in label:\n",
    "            #     match_details['Series'] = value\n",
    "            elif 'Season' in label:\n",
    "                match_details['Season'] = value\n",
    "            elif 'Player Of The Match' in label:\n",
    "                match_details['Player Of The Match'] = value\n",
    "            elif 'Match days' in label:\n",
    "                match_date = value.split(' ')[:3]\n",
    "                match_details['Match_Day'] = ' '.join(match_date)\n",
    "            elif 'Umpires' in label:\n",
    "                umpires = [a.text.strip() for a in cols[1].find_all('a')]\n",
    "                match_details['Umpires'] = umpires\n",
    "            # elif 'TV Umpire' in label:\n",
    "            #     match_details['TV Umpire'] = value\n",
    "            # elif 'Points' in label:\n",
    "            #     match_details['Points'] = value\n",
    "    match_info_p = soup.find('p', {'class': 'ds-text-tight-s ds-font-medium ds-truncate ds-text-typo'})\n",
    "    if match_info_p:\n",
    "        match_details['Match Info'] = match_info_p.text.strip()\n",
    "        \n",
    "    team_divs = soup.find_all('div', {'class': 'ds-text-tight-m ds-font-bold ds-text-typo'})\n",
    "    if len(team_divs) >= 2:  # Ensure two teams are present\n",
    "        match_details['Team 1'] = team_divs[0].text.strip()\n",
    "        match_details['Team 2'] = team_divs[1].text.strip()\n",
    "    target_element = soup.find('div', {'class': 'ds-text-compact-m ds-text-typo ds-text-right ds-whitespace-nowrap'})\n",
    "    if target_element:\n",
    "        target_value = target_element.find('strong').text.strip()\n",
    "        match_details['Target'] = target_value\n",
    "    else:\n",
    "        match_details['Target'] = 'Not Available'\n",
    "    \n",
    "    \n",
    "    #1st teeam`s run and wic lost by 1st one team\n",
    "    match_details['Target_Runs'] = int(target_value.split('/')[0]) + 1 if '/' in target_value else int(target_value) + 1\n",
    "    match_details['Wic_lost_by_1st_team'] = target_value.split('/')[1] if '/' in target_value else 10 \n",
    "    \n",
    "    \n",
    "    # Extract runs scored by the second team (use a different element or class if necessary)\n",
    "    runs_scored_element = soup.find_all('div', {'class': 'ds-text-compact-m ds-text-typo ds-text-right ds-whitespace-nowrap'})\n",
    "    if len(runs_scored_element) > 1:  # Assuming the second div contains runs scored by second team\n",
    "        runs_scored = runs_scored_element[1].find('strong').text.strip()\n",
    "        match_details['Second_Team_Performance'] = runs_scored\n",
    "        match_details['Run_Scored_By_2nd_Team'] = runs_scored.split('/')[0]\n",
    "        match_details['Wic_Lost_by_2nd_Team'] = runs_scored.split('/')[1] if '/' in runs_scored else 10\n",
    "    else:\n",
    "        match_details['Runs_Scored_By_Second_Team'] = 'Not Available'\n",
    "    \n",
    "    # Extract the run rates for both teams\n",
    "    run_rates = re.findall(r'\\(RR:\\s([\\d.]+)\\)', driver.page_source)\n",
    "    if len(run_rates) >= 2:\n",
    "        match_details['Run_Rate_Team_1'] = run_rates[0]\n",
    "        match_details['Run_Rate_Team_2'] = run_rates[1]\n",
    "    else:\n",
    "        match_details['Run_Rate_Team_1'] = 'Not Available'\n",
    "        match_details['Run_Rate_Team_2'] = 'Not Available'\n",
    "    \n",
    "    \n",
    "    \n",
    "    return match_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(r'C:\\\\Users\\\\Roshan Koirala\\\\Desktop\\\\FUN with GPT\\\\NEPAL PREMIER LEAGUE\\\\NPL NEW TRY\\\\geckodriver-v0.35.0-win64 (1)\\\\geckodriver.exe')\n",
    "\n",
    "driver = webdriver.Firefox(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_details = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_powerplay_info(url):\n",
    "    try:\n",
    "        # Fetch the HTML content from the match URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the <span> with the Powerplay info\n",
    "        powerplay_spans = soup.find_all('span', string=lambda text: text and 'Powerplay' in text)\n",
    "\n",
    "        if len(powerplay_spans) >= 2:\n",
    "            # Get the first Powerplay info (first team's Powerplay performance)\n",
    "            powerplay_1st_team_text = powerplay_spans[0].get_text()\n",
    "            start_index_1st = powerplay_1st_team_text.find(\"Mandatory\")\n",
    "            if start_index_1st != -1:\n",
    "                powerplay_1st_team_performance = powerplay_1st_team_text[start_index_1st:].split(\"Mandatory - \")[1].strip()\n",
    "                match_details['powerplayperformance_1st_team'] = powerplay_1st_team_performance\n",
    "            else:\n",
    "                match_details['powerplayperformance_1st_team'] = \"Powerplay info not found\"\n",
    "\n",
    "            # Get the second Powerplay info (second team's Powerplay performance)\n",
    "            powerplay_2nd_team_text = powerplay_spans[1].get_text()\n",
    "            start_index_2nd = powerplay_2nd_team_text.find(\"Mandatory\")\n",
    "            if start_index_2nd != -1:\n",
    "                powerplay_2nd_team_performance = powerplay_2nd_team_text[start_index_2nd:].split(\"Mandatory - \")[1].strip()\n",
    "                match_details['powerplayperformance_2nd_team'] = powerplay_2nd_team_performance\n",
    "            else:\n",
    "                match_details['powerplayperformance_2nd_team'] = \"Powerplay info not found\"\n",
    "        else:\n",
    "            match_details['powerplayperformance_1st_team'] = \"Powerplay info not found\"\n",
    "            match_details['powerplayperformance_2nd_team'] = \"Powerplay info not found\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        match_details['error'] = f\"Error fetching Powerplay info: {e}\"\n",
    "    \n",
    "    return match_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches = []\n",
    "\n",
    "# for url in npl_matches:\n",
    "#     match_details = scrape_match_details(driver, url)\n",
    "\n",
    "#     match_info = match_details.get('Match Info', '')\n",
    "\n",
    "#     if match_info:\n",
    "#         match_info_parts = match_info.split(' by ')\n",
    "        \n",
    "#         if len(match_info_parts) == 2:  # Ensure 'by' splitting gives two parts\n",
    "#             winner_team = match_info_parts[0]\n",
    "#             won_by_margin = match_info_parts[1].strip()\n",
    "            \n",
    "#             if winner_team.startswith('S'):\n",
    "#                 match_details['Winner_Team'] = ' '.join(winner_team.split(' ')[:2])\n",
    "#             else:\n",
    "#                 match_details['Winner_Team'] = winner_team.split(' ')[0]\n",
    "            \n",
    "#             match_details['won_by_margin'] = won_by_margin\n",
    "    \n",
    "       \n",
    "       \n",
    "#        ## powerplay\n",
    "            \n",
    "        \n",
    "#     matches.append(match_details)\n",
    "#     print(match_details)\n",
    "\n",
    "\n",
    "#THE USEFUL CODEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "\n",
    "\n",
    "matches = []\n",
    "\n",
    "for url in npl_matches:\n",
    "    match_details = scrape_match_details(driver, url)\n",
    "\n",
    "    match_info = match_details.get('Match Info', '')\n",
    "\n",
    "    if match_info:\n",
    "        match_info_parts = match_info.split(' by ')\n",
    "        \n",
    "        if len(match_info_parts) == 2:  # Ensure 'by' splitting gives two parts\n",
    "            winner_team = match_info_parts[0]\n",
    "            won_by_margin = match_info_parts[1].strip()\n",
    "            \n",
    "            if winner_team.startswith('S'):\n",
    "                match_details['Winner_Team'] = ' '.join(winner_team.split(' ')[:2])\n",
    "            else:\n",
    "                match_details['Winner_Team'] = winner_team.split(' ')[0]\n",
    "            \n",
    "            match_details['won_by_margin'] = won_by_margin\n",
    "    \n",
    "    ## Extract Powerplay info\n",
    "    try:\n",
    "        # Assuming you have a function to get the HTML content and parse the Powerplay info\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the Powerplay info using the 'Powerplay' text\n",
    "        powerplay_info = soup.find('span', string=lambda text: text and 'Powerplay' in text)\n",
    "\n",
    "        if powerplay_info:\n",
    "            match_details['Powerplay_Info'] = powerplay_info.get_text()\n",
    "            \n",
    "        else:\n",
    "            match_details['Powerplay_Info'] = \"Powerplay info not found\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        match_details['Powerplay_Info'] = f\"Error fetching Powerplay info: {e}\"\n",
    "\n",
    "    # Append the match details to the list\n",
    "    matches.append(match_details)\n",
    "\n",
    "    # Print the match details including Powerplay info\n",
    "    print(match_details)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN FUNCTION FOR POWERPLAYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
    "\n",
    "# def extract_powerplay_info(url):\n",
    "#     try:\n",
    "#         # Fetch the HTML content from the match URL\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#         # Find the <span> with the Powerplay info\n",
    "#         powerplay_info = soup.find('span', string=lambda text: text and 'Powerplay' in text)\n",
    "\n",
    "#         if powerplay_info:\n",
    "#             return powerplay_info.get_text()\n",
    "#         else:\n",
    "#             return \"Powerplay info not found\"\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         return f\"Error fetching data for {url}: {e}\"\n",
    "\n",
    "# # Loop through each match URL and extract the Powerplay info\n",
    "# all_powerplay_info = []\n",
    "\n",
    "# for match_url in npl_matches:\n",
    "#     info = extract_powerplay_info(match_url)\n",
    "#     match_details['PowerPlay'] = info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_name = \"Nepal_Premier_League.csv\"\n",
    "\n",
    "if matches:\n",
    "    headers = matches[0].keys()  \n",
    "\n",
    "with open(csv_file_name, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "    writer.writeheader()  \n",
    "    writer.writerows(matches)  \n",
    "\n",
    "print(f\"Match details have been written to {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Nepal_Premier_League.csv')\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
